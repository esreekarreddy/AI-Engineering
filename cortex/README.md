# Cortex

> **Live Demo:** [sr-cortex.vercel.app](https://sr-cortex.vercel.app)

A multi-agent code review system that runs locally using Ollama. Six specialized AI agents analyze your code from different perspectivesâ€”architecture, security, performance, and moreâ€”then produce ranked, actionable findings.

## ğŸŒ Overview

Cortex convenes a "council" of AI specialists to review your code. Each agent has a distinct role and preferred model, producing findings that are cross-validated and ranked by severity. The result is a comprehensive code review that would typically require multiple human experts.

## ğŸ¤– AI Integration

**Runtime:** Ollama (local inference server)

**Agent Configuration:**

| Agent          | Specialty     | Default Model     | Purpose                                 |
| -------------- | ------------- | ----------------- | --------------------------------------- |
| **Moderator**  | Orchestration | mistral-small3.2  | Coordinates review, synthesizes verdict |
| **Architect**  | Design        | mistral-small3.2  | Structure, patterns, maintainability    |
| **Sentinel**   | Security      | deepseek-coder-v2 | Bugs, vulnerabilities, edge cases       |
| **Optimizer**  | Performance   | phi4              | Bottlenecks, complexity, efficiency     |
| **Maintainer** | Quality       | mistral-small3.2  | Tests, error handling, DX               |
| **Verifier**   | Validation    | deepseek-r1       | Cross-checks claims against code        |

**Technical Details:**

- Streaming responses with real-time chat updates
- `keep_alive: 0` for immediate model unloading (memory efficiency)
- Automatic fallback to available models if preferred model is missing
- Configurable temperature and context window per agent

## âœ¨ Features

### ğŸ›ï¸ Multi-Agent Council

- **Parallel Analysis** â€” Agents review concurrently for faster results
- **Specialized Perspectives** â€” Each agent focuses on their domain expertise
- **Cross-Validation** â€” Verifier challenges claims from other agents
- **Synthesized Verdict** â€” Moderator produces final ranked output

### ğŸ¯ Intelligent Findings

- **Severity Ranking** â€” P0 (Critical) to P3 (Minor) classification
- **Evidence-Based** â€” Each finding cites specific code locations
- **Actionable Fixes** â€” Concrete patch snippets included
- **Trade-off Analysis** â€” Notes potential downsides of suggested changes

### ğŸ’¾ Memory Optimized

- **Sequential Unloading** â€” Models freed after each agent completes
- **8GB Viable** â€” Works with smaller models on limited hardware
- **Large Model Support** â€” 32GB+ enables deepseek-coder-v2, llama3.3:70b

### ğŸ¨ Developer Experience

- **Monaco Editor** â€” VS Code-grade code input with syntax highlighting
- **Real-Time Chat** â€” Watch agents discuss in the council panel
- **Verdict Dashboard** â€” Sortable findings with severity badges
- **Dark UI** â€” Glassmorphism design with agent-colored accents

## ğŸš€ Getting Started

### Prerequisites

1. **Install Ollama** â€” [ollama.ai/download](https://ollama.ai/download)
2. **Pull models:**
   ```bash
   ollama pull mistral
   ollama pull deepseek-coder
   ollama pull phi3
   ```
3. **Start Ollama:**
   ```bash
   ollama serve
   ```

### Run Cortex

```bash
# Install dependencies
npm install

# Start dev server
npm run dev

# Open browser
open http://localhost:3000
```

## ğŸ“– Usage

1. **Paste code** into the Monaco editor
2. **Click "Review"** to convene the council
3. **Watch agents** discuss in the Chat panel
4. **Review findings** in the Verdict panel (sorted by severity)
5. **Apply fixes** based on agent suggestions

## ğŸ—ï¸ Architecture

```
cortex/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/ollama/       # Ollama proxy bridge with streaming
â”‚   â”‚   â””â”€â”€ page.tsx          # Main council UI
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ council/          # AgentCard, CouncilChat, VerdictPanel
â”‚   â”‚   â”œâ”€â”€ editor/           # Monaco code editor
â”‚   â”‚   â””â”€â”€ ui/               # Badge, CyberButton, GlassPanel
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ agents/
â”‚       â”‚   â”œâ”€â”€ orchestrator.ts  # Council coordination logic
â”‚       â”‚   â”œâ”€â”€ prompts.ts       # Agent-specific system prompts
â”‚       â”‚   â””â”€â”€ types.ts         # Agent/Finding TypeScript types
â”‚       â”œâ”€â”€ ollama/
â”‚       â”‚   â””â”€â”€ client.ts        # Ollama API wrapper
â”‚       â””â”€â”€ store.ts             # Zustand global state
```

## ğŸ’» System Requirements

| RAM   | Capability                                     |
| ----- | ---------------------------------------------- |
| 8GB   | Small models (phi3, llama3:8b)                 |
| 16GB  | Medium models (mistral, llama3)                |
| 32GB+ | Large models (deepseek-coder-v2, llama3.3:70b) |

## ğŸ”’ Security Notes

- **100% Local** â€” Code never leaves your machine
- **No External APIs** â€” All inference via localhost:11434
- **Open Source** â€” Full codebase transparency

---

_Built by [Sreekar Reddy](https://github.com/esreekarreddy)_
