# SR Nexus

**Advanced MCP Agent Debugging Platform**

> A cinematic, interactive platform for building, testing, and debugging MCP-powered AI agents with real-time decision tree visualization, synthetic enterprise environments, and comprehensive debugging tools.

## ğŸš€ Live Demo

[Coming Soon]

## ğŸ“¸ Screenshots

_Coming Soon_

## âœ¨ Features

### Core Capabilities

- **Real-time Decision Tree Visualization** â€” Watch AI reasoning unfold as an interactive graph
- **Synthetic Enterprise Environment** â€” Pre-built fake services (Tickets, Wiki) with 100+ items
- **Model Switching** â€” Compare Mistral 7B, Llama 3.1, and FunctionGemma performance
- **MCP Protocol Native** â€” Full Model Context Protocol support

### Advanced Features

- **Time-Travel Debugging** â€” Replay and branch from any execution point
- **Multi-Agent Orchestration** â€” Run multiple agents collaborating on tasks
- **Performance Profiler** â€” Token usage, latency breakdown, bottleneck detection
- **Evaluation Suite** â€” Grade agent correctness against ground truth

### Developer Experience

- **Server Browser** â€” Explore all synthetic MCP servers
- **Tools Explorer** â€” View all tool definitions and schemas
- **Resources Viewer** â€” Browse available resources and metadata
- **Prompts Library** â€” Test prompt templates with custom inputs

## ğŸ› ï¸ Tech Stack

| Layer         | Technology                |
| ------------- | ------------------------- |
| Framework     | Next.js 16 (App Router)   |
| Language      | TypeScript                |
| Styling       | Tailwind CSS v4           |
| State         | Zustand                   |
| Visualization | React Flow                |
| Editor        | Monaco Editor             |
| Charts        | Recharts                  |
| Animation     | Framer Motion             |
| MCP           | @modelcontextprotocol/sdk |
| LLM           | Ollama (Local)            |

## ğŸ“‹ Prerequisites

- Node.js 18+
- Ollama installed locally
- One of the following models:
  - `ollama pull mistral:7b` (Recommended - fastest)
  - `ollama pull llama3.1:8b` (Best accuracy)
  - `ollama pull functiongemma` (Experimental)

## ğŸš€ Getting Started

```bash
# Clone the repository
git clone https://github.com/esreekarreddy/AI-Engineering.git
cd AI-Engineering/nexus

# Install dependencies
npm install

# Start Ollama (in another terminal)
ollama serve

# Run development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000)

## ğŸ“– Documentation

See [SPEC.md](./SPEC.md) for detailed specifications and architecture.

## ğŸ”’ Privacy

- **100% Local** â€” All AI inference runs on your machine via Ollama
- **No Cloud APIs** â€” Zero external API calls for LLM
- **Local Storage** â€” All traces and history stored in browser
- **Reset Option** â€” Clear all data with one click

## ğŸ“œ License

MIT

## ğŸ‘¤ Author

**Sreekar Reddy**

- Portfolio: [sreekarreddy.com](https://sreekarreddy.com)
- GitHub: [@esreekarreddy](https://github.com/esreekarreddy)
